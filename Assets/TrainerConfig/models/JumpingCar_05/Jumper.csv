Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,High Score,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,0.63486725,15.772041568890378,-0.58699423,-0.9109322529693404,-0.9109322529693404,13.0,0.052586384,0.025302278,0.0002846291,0.19487636,0.0047443295,1.0
100000,0.4062271,34.66881258941345,-0.76137537,-0.9237482034586615,-0.9237482034586615,None,0.015744323,0.023195198,0.000256936,0.18564531,0.0042837015,1.0
150000,0.220312,70.48550724637681,-0.67047936,-0.9053545505045637,-0.9053545505045637,None,0.01436138,0.02312516,0.00022612672,0.17537558,0.003771241,1.0
200000,0.11530623,127.65714285714286,-0.5343238,-0.8690909023602288,-0.8690909023602288,None,0.014532809,0.024004985,0.00019528865,0.1650962,0.0032583005,1.0
250000,0.086800314,248.61046511627907,-0.38119933,-0.6575581299271002,-0.6575581299271002,26.0,0.013554168,0.024018075,0.00016446822,0.1548227,0.0027456537,1.0
300000,0.06410569,795.8333333333334,-0.16975,0.4238095588627316,0.4238095588627316,92.0,0.005809568,0.026078124,0.00013364264,0.1445475,0.002232921,1.0
350000,0.06049449,1445.1052631578948,-0.024614459,1.7157895412099988,1.7157895412099988,169.0,0.00392851,0.020309547,0.00010286046,0.1342868,0.0017209111,1.0
400000,0.058244344,2522.1176470588234,0.045557965,3.6941177419879856,3.6941177419879856,218.0,0.0037350594,0.023614466,7.514213e-05,0.12504736,0.0012598626,1.0
450000,0.056292847,526.7307692307693,0.098716244,0.015384654299570965,0.015384654299570965,297.0,0.004645371,0.023398202,4.738473e-05,0.11579488,0.0007981645,1.0
